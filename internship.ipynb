{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"internship.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOFNgpJlgUh5outDgT5w9aa"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"z2lLc7DQJagJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":490},"executionInfo":{"status":"ok","timestamp":1594034632457,"user_tz":-330,"elapsed":7608,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}},"outputId":"0aab2575-7bed-4dc1-cf8c-884f5ad09deb"},"source":["import spacy \n","from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords \n","from sklearn.feature_extraction.text import CountVectorizer \n","from sklearn.metrics import accuracy_score \n","from sklearn.base import TransformerMixin \n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import LinearSVC\n","import string\n","punctuations = string.punctuation\n","!python3 -m spacy download en\n","spacy.load('en_core_web_sm')\n","from spacy.lang.en import English\n","parser = English()\n","import pandas as pd\n","import random\n","import inflect\n","p = inflect.engine()\n","\n"],"execution_count":160,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (47.3.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n","/usr/local/lib/python3.6/dist-packages/spacy/data/en\n","You can now load the model via spacy.load('en')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-pku02cGJkdE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594034632459,"user_tz":-330,"elapsed":7597,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}}},"source":["def datapreprocessing(url):\n","\n","  # Load the dataset into a pandas dataframe.\n","  df = pd.read_csv(url, delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n","\n","  sentences = df.sentence.values\n","  labels = df.label.values\n","  train=[]\n","  for i in range(len(df)):\n","    train.append((df.sentence[i],df.label[i]))\n","  \n","  random.shuffle(train)\n","\n","  train_data = train[:7000]\n","  test_data = train[7000:]  \n","  return train_data,test_data"],"execution_count":161,"outputs":[]},{"cell_type":"code","metadata":{"id":"8h9bHg1cK6yg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594034632461,"user_tz":-330,"elapsed":7591,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}}},"source":["train_data,test_data=datapreprocessing('/in_domain_train.tsv')"],"execution_count":162,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fpn7Cu-VK_rb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594034632462,"user_tz":-330,"elapsed":7584,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}}},"source":["def model():\n","  #Custom transformer using spaCy \n","  class predictors(TransformerMixin):\n","      def transform(self, X, **transform_params):\n","          return [clean_text(text) for text in X]\n","      def fit(self, X, y=None, **fit_params):\n","          return self\n","      def get_params(self, deep=True):\n","          return {}\n","\n","  # Basic utility function to clean the text \n","  def clean_text(text):     \n","      return text.strip()\n","  def spacy_tokenizer(sentence):\n","      tokens = parser(sentence)\n","      tokens = [ tok for tok in tokens]\n","      return tokens\n","\n","  #create vectorizer object to generate feature vectors, we will use custom spacy tokenizer\n","  vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)) \n","  classifier = LinearSVC()\n","\n","  # Create the  pipeline to clean, tokenize, vectorize, and classify \n","  pipe = Pipeline([(\"cleaner\", predictors()),('vectorizer', vectorizer),('classifier', classifier)])\n","  \n","  return pipe"],"execution_count":163,"outputs":[]},{"cell_type":"code","metadata":{"id":"aw64GfM8LB5D","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594034632464,"user_tz":-330,"elapsed":7578,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}}},"source":["def detection(sen):\n","  pipe=model()\n","  pipe.fit([x[0] for x in train_data], [x[1] for x in train_data]) \n","  pred_data = pipe.predict(sen) \n","  result=pred_data[0]\n","  return result"],"execution_count":164,"outputs":[]},{"cell_type":"code","metadata":{"id":"-lBh_XSASh7v","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594034632466,"user_tz":-330,"elapsed":7571,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}}},"source":["def punct_data(sen):\n","  # Load the dataset into a pandas dataframe.\n","  df = pd.read_csv('/punct.csv')\n","  \n","  sentences = df.sentence.values\n","  labels = df.lables.values\n","  train=[]\n","  for i in range(len(df)):\n","    train.append((df.sentence[i],df.lables[i]))\n","  \n","  random.shuffle(train)\n","\n","  train_data = train[:200]\n","  test_data = train[200:]  \n","  class predictors(TransformerMixin):\n","      def transform(self, X, **transform_params):\n","          return [clean_text(text) for text in X]\n","      def fit(self, X, y=None, **fit_params):\n","          return self\n","      def get_params(self, deep=True):\n","          return {}\n","  def clean_text(text):     \n","      return text.strip().lower()\n","  def spacy_tokenizer(sentence):\n","      tokens = parser(sentence)\n","      tokens = [ tok.lower_ for tok in tokens]\n","      return tokens\n","\n","  #create vectorizer object to generate feature vectors, we will use custom spacy tokenizer\n","  vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)) \n","  classifier = LinearSVC()\n","\n","  # Create the  pipeline to clean, tokenize, vectorize, and classify \n","  pipe = Pipeline([(\"cleaner\", predictors()),('vectorizer', vectorizer),('classifier', classifier)])\n","  pipe.fit([x[0] for x in train_data], [x[1] for x in train_data]) \n","  pred_data = pipe.predict(sen) \n","  result=pred_data[0]\n","  print(result)\n","  return result"],"execution_count":165,"outputs":[]},{"cell_type":"code","metadata":{"id":"kN7p2h_sokiJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1594034635314,"user_tz":-330,"elapsed":10409,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}},"outputId":"e13a18dd-5b04-461a-b4c3-c860ade93184"},"source":["import spacy.cli\n","spacy.cli.download(\"en_core_web_lg\")"],"execution_count":166,"outputs":[{"output_type":"stream","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"avLOkzVYSiD2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594034635317,"user_tz":-330,"elapsed":10403,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}}},"source":["def article(sen):\n","  import nltk\n","  import en_core_web_lg\n","  nlp = en_core_web_lg.load()\n","  doc = nlp(sen)\n","  art_word=[]\n","  art=[]\n","  fl=0\n","  article=['a','an']\n","  for i,word in enumerate(doc):\n","    if(fl==1):\n","      art_word.append(word)\n","      fl=0\n","    st=str(word)\n","    if st in article:\n","      art.append(st)\n","      fl=1\n","      pass\n","  for i,word in enumerate(art_word):\n","    st=str(word)\n","    q=p.a(st)\n","    if((art[i]+\" \"+st)==q):\n","      return 1\n","    else:\n","      return 0\n","  "],"execution_count":167,"outputs":[]},{"cell_type":"code","metadata":{"id":"emFEeKxeSiIz","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594034643596,"user_tz":-330,"elapsed":1021,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}}},"source":["def capital(sen):\n","  if sen[0].islower():\n","    print(\"Incorrect Capitalization\")\n","  "],"execution_count":169,"outputs":[]},{"cell_type":"code","metadata":{"id":"SUGqyk3dLa8y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1594034664193,"user_tz":-330,"elapsed":18371,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}},"outputId":"e58c6a32-d2c2-47e6-8ed6-30679a85a75a"},"source":["sen=[]\n","sen.append(input(\"Enter Sentence\"))\n","result=detection(sen[0])\n","if result==1:\n","  print(\"Sentence is Grammetically Correct\")\n","else:\n","  print(\"Sentence is Grammetically Incorrect\")\n","  punct=punctuation(sen[0])  \n","  if punct==0:\n","    print(\"Incorrect Punctuations\")\n","  art=article(sen[0])  \n","  if art==0:\n","      print(\"Incorrect Article(a/an)\")  \n","  capital(sen[0])   "],"execution_count":170,"outputs":[{"output_type":"stream","text":["Enter SentenceHello ,how are you?\n","Sentence is Grammetically Correct\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aoA5JJukMWmv","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1594034635322,"user_tz":-330,"elapsed":10355,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecmfEdv3QxdZ","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1594034635323,"user_tz":-330,"elapsed":10347,"user":{"displayName":"Samira Shaikh","photoUrl":"","userId":"10763523033781727261"}}},"source":[""],"execution_count":null,"outputs":[]}]}